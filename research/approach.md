# Team Approach
Final Pipeline Validation and Implementation Plan

Pipeline Decisions (Validated)
	•	Model Choice – DistilBERT vs. RoBERTa: We will fine-tune DistilBERT (base, uncased) as our text encoder. DistilBERT is a small, fast transformer that retains over 95% of BERT-base’s accuracy on language tasks while using ~40% fewer parameters ￼ ￼. This makes it ideal for a ~3k dataset on Mac hardware, giving strong performance without the heavy training time of RoBERTa-base. We acknowledge that RoBERTa-base could offer a slight accuracy boost, but it’s twice the size and slower. We prefer speed and simplicity for our 2-week timeline. (If needed later, we can try a RoBERTa variant – e.g. distilroberta-base – to see if it improves validation accuracy.)
	•	Input Features – All Text, No Complex Fusion: All informative fields will be combined as text: recipe name, ingredients, tags, description, and steps. This leverages nearly all the data in natural language form. We will not add separate numeric or categorical branches (like one-hot encoding for tags or date features) in the initial model. Including tags as plain text (e.g. “Tags: vegan, dessert”) feeds that info into DistilBERT’s understanding. Numeric fields like n_ingredients or date are initially skipped to avoid complicating the model – these add minimal signal and could introduce noise. (If we discover later that a field like n_ingredients has predictive value, we can inject it as a simple text token or a small extra input, but it’s likely unnecessary given the rich textual data.) Overall, a text-only approach aligns with Javier’s suggestion and keeps the pipeline lean.
	•	Sequence Length & Field Ordering: We set a generous max_length=512 for tokenization, using the DistilBERT tokenizer. Our analysis shows ~98% of recipes fit within 512 tokens, with a median length ~272. This means truncation will rarely drop information. We will truncate from the end (the recipe steps last) if needed. To further reduce any truncation impact, we order the concatenated fields by importance: Name → Ingredients → Tags → Description → Steps. This ensures the most crucial info (title, ingredients, tags) appears first in the sequence. The steps (often the longest, ~42% of tokens) come last and will be the first part truncated if a recipe is very long. Using Hugging Face’s tokenizer, we’ll apply truncation=True and padding='longest' so that within each batch we pad only to the longest example. This dynamic padding saves memory and computation (padding to 280 tokens on average instead of always 512). We choose 512 tokens instead of a lower cap like 384 because ~10% of recipes would be truncated at 384. Given our batch-wise padding, the memory cost of allowing up to 512 is moderate, and we preserve more information per sample. If we encounter memory issues or slow training, we can revisit this (e.g. try 384 and see if accuracy stays high).
	•	Classifier Head – Simple Linear Layer: On top of DistilBERT’s [CLS] representation, we use a single fully-connected layer mapping from the 768-dim hidden state to 6 chef classes. This means the model’s output for classification is essentially a linear projection of the [CLS] embedding (with built-in dropout from DistilBERT). This minimalist head is chosen to avoid overfitting on only ~3000 samples. It’s a common practice to use just one linear layer for BERT fine-tuning ￼ ￼; the transformer’s own layers have likely separated the classes well enough that a complex multi-layer head isn’t needed. By keeping the head shallow, we minimize new parameters and hyperparameters. (We can always experiment with a deeper head (e.g. add a hidden layer with GELU activation) if the linear head underperforms, but initially “less is more.”)
	•	Optimization & Regularization: We will fine-tune the entire DistilBERT model (no frozen layers) using AdamW optimizer. A single learning rate of 2e-5 for all parameters will be used for simplicity – this is a standard starting point for transformer fine-tuning. We opt not to complicate with multiple learning rates (e.g. higher LR for the head) to reduce hyperparameter tuning. Weight decay around 0.01 will be applied to regularize. We’ll train for about 3 to 5 epochs, with early stopping on validation performance (patience 1–2 epochs) to prevent overfitting. Given the small dataset, the model may converge quickly (possibly by epoch 2 or 3), so early stopping is important. We’ll also use a short learning rate warmup (around 5% of training steps) as is common in transformer fine-tuning, to stabilize the initial updates. Batch size will be 16 for training (to fit in memory on M1/M2 GPU via MPS) and 32 for evaluation. We fix a random seed (e.g. 42) to ensure reproducible splits and results. This training setup is a straightforward, proven recipe for fine-tuning transformers.
	•	Evaluation Metrics – Accuracy and Macro-F1: Our primary metric will be accuracy on the validation set, since the final goal is to maximize overall correct predictions. However, given the class imbalance (one chef has ~806 recipes vs another with ~372), we will also track Macro F1 score. Macro F1 calculates the F1 (balance of precision/recall) for each chef and averages them, thus treating all classes equally ￼. This is crucial to ensure the model isn’t only doing well on the majority class – we want it to perform reasonably across all six chefs. A high macro-F1 will indicate the model is not neglecting the smaller classes. We won’t explicitly use class weighting at first (the imbalance ratio ~2.17 isn’t extreme), but if we notice the model predicting only the majority chef, we could consider class weights or re-sampling. In summary, success means higher accuracy than a TF-IDF baseline and solid Macro-F1 (no chef left behind).
	•	Choices from Colleagues – “Best of Both Worlds”: Our plan intentionally combines the best ideas from the team while avoiding over-engineering. From Maxence’s proposal, we agreed with using Macro-F1 for imbalanced evaluation and considered his idea of a GELU-activated dense layer – but we’ll hold off on adding it unless needed. We decided not to include Maxence’s structured feature pipeline (date encoding, separate MLP for n_ingredients and tags) initially. The rationale is that these add complexity for uncertain gain: the tags are already integrated as text, the date likely has weak correlation with chef identity, and an extra numeric MLP could overfit our small data. We prefer to get a strong text-only model first. From Javier’s plan, we adopted the text field ordering and formatting, as well as his focus on a single-model text approach. Javier’s use of roberta-base is noted, but as discussed, we’ll start with DistilBERT for efficiency. We’ll also follow his suggestion of light text normalization (we won’t aggressively clean or lowercase beyond what the tokenizer does). Both colleagues suggested baseline comparisons and small ablation studies, which we’ve included in our process. Overall, the plan is to start simple, using the combined insights, and only add complexity if the simpler approach falls short.

Implementation Plan (Step-by-Step)

We will execute the project in clear stages, ensuring at each step that we validate our choices and gather any needed information:

Stage 1: Data Exploration and Preparation

Tasks: Begin by examining the dataset and preparing it for modeling. Load the CSV data (recipes with their fields and chef labels) and perform an exploratory analysis. Calculate basic stats: the number of recipes, class counts per chef, and distribution of text lengths (tokens per recipe when concatenated). This confirms the imbalance ratio (about 2.17x between largest and smallest class) and typical recipe lengths (median ~272 tokens, most under 512). Document any observations (e.g., which chef is most frequent, any quirks in text data).
Outcome: A clear understanding of data characteristics that justify our modeling decisions (for example, knowing 98% of recipes fit in 512 tokens confirms our max_length choice). Also, prepare the dataset for modeling: clean any obvious noise (trimming extra whitespace or stray HTML if present), but do not remove stopwords or casing (the transformer tokenizer will handle that appropriately). Finally, perform a stratified split of the data into train and validation sets (e.g., 80/20). Stratification by chef ensures each set has a proportional representation of all 6 chefs, which is essential given the imbalance. Save these splits for consistent use in training and evaluation.

Stage 2: Feature Construction (Text Concatenation)

Tasks: Construct the input text for the model from the selected fields. For each recipe, concatenate the fields in the decided order: Name, Ingredients, Tags, Description, Steps. Use clear separators or labels (e.g., "Ingredients: ... Tags: ... Description: ... Steps: ...") to help the model distinguish sections. This can be done in code when creating the dataset or on-the-fly in a custom dataset class. Ensure that important fields like name and ingredients come first in the string. We will not include the date or numeric fields, to keep inputs purely textual. Once concatenated, tokenize the text using DistilBERT’s tokenizer. Configure the tokenizer with max_length=512 and truncation=True (so longer texts are cut off at 512 tokens) and padding='longest' when batching. It’s a good practice to verify the tokenization on a few examples (check that our special tokens and separators look correct and that truncation is happening only for the longest recipes).
Outcome: A processed dataset where each recipe is converted into a sequence of input IDs ready for the model. We’ll have PyTorch Dataset/DataLoader objects or use Hugging Face’s Dataset API to feed data into our model training loop. This stage yields the code to go from raw CSV to tokenized batches.

Stage 3: Baseline Model Evaluation

Tasks: Before fine-tuning BERT, run a simple baseline to have a point of comparison. A common baseline is a TF-IDF bag-of-words representation of the text with a straightforward classifier (e.g., Logistic Regression). We will take the concatenated text for each recipe (possibly a simplified version, like just ingredients or name + ingredients, to mimic a simple approach) and compute TF-IDF features. Then train a logistic regression or linear SVM to predict the chef. Evaluate this baseline on the validation set, recording the accuracy (and perhaps macro-F1). This gives us a benchmark (e.g., say the baseline accuracy is around 40-45%). Ensure to use the same train/val split for a fair comparison.
Outcome: A baseline accuracy/Macro-F1 to beat. This also double-checks that our data pipeline is correct (if the baseline struggles to learn, we might spot issues in data labels or preprocessing). The baseline results will be referenced in the report to demonstrate the value of our BERT-based approach.

Stage 4: Fine-Tune DistilBERT Model

Tasks: Set up the DistilBERT fine-tuning training loop. This involves initializing the distilbert-base-uncased model with a classification head. We can use Hugging Face’s AutoModelForSequenceClassification for convenience, which by default will create a linear layer on top of [CLS] for the number of classes we specify. Double-check that the model’s output dimension is 6 (for 6 chefs). Next, configure the training hyperparameters: use AdamW optimizer (with weight decay 0.01) and set the learning rate to 2e-5 for all model parameters. Set up a learning rate scheduler with a small warmup (e.g., warmup steps = 5% of total steps) to avoid initial instability. We train for up to 5 epochs, but we will incorporate early stopping. Monitor the validation accuracy (and macro-F1) each epoch. If using Hugging Face’s Trainer API, we can specify an early stopping callback; otherwise, we manually break out if validation metrics don’t improve after 1-2 epochs. Also enable model checkpointing to save the best model weights (based on val accuracy) so far. Training will be done on Apple Silicon GPU (using torch.compile/MPS) or CPU if necessary – verify that the batch size of 16 is working memory-wise. During training, watch for signs of overfitting: if training loss keeps dropping but val accuracy stalls or drops, that’s our cue to stop early.
Outcome: A trained DistilBERT classifier that likely outperforms the baseline. We expect significantly higher accuracy (maybe in the 60-70% range or more, based on similar text-classification improvements). We will have a saved best model checkpoint from the epoch with highest val accuracy. We’ll also note the training time and ensure it’s within our project limits.

Stage 5: Validation and Performance Analysis

Tasks: Evaluate the fine-tuned model on the validation set in detail. Calculate the accuracy and macro-F1 on val to see if we met our goals (did we beat the baseline by a good margin? Are all classes getting reasonable F1 scores?). Additionally, produce a confusion matrix or per-class precision/recall to identify if any chef class is still being misclassified frequently. This analysis will tell us if further improvements are needed. For example, if one chef (especially the smaller one) has very low recall, we might consider strategies like class weighting or collecting more data (if possible) or checking if that class has any pattern we missed. Also review some example predictions versus truth to qualitatively assess errors (are certain chefs getting confused with each other? Perhaps their recipes are similar).
Outcome: A clear picture of how well our model performs and which areas (if any) need attention. At this point, if the model meets the success criteria (e.g., far above baseline and acceptable macro-F1), we can proceed to finalizing. If not, we decide on targeted adjustments in the next stage.

Stage 6: Iterations and Enhancements (If Needed)

Tasks (Optional, as needed): Based on the validation results, consider some enhancements:
	•	Field Ablation Experiments: We plan quick ablations to justify our design. This means training variants of the model with some fields removed from the input to see the impact. For example, train a model using only Name+Ingredients+Tags, another with Name+Ingredients+Tags+Description, and compare these to the full input (which includes Steps). These can be done with fewer epochs just to see relative validation accuracy. If we observe a big drop when steps are removed, that confirms steps field was contributing; if not, perhaps steps were mostly noise and we could have truncated more aggressively. These findings will be reported to back up our choices.
	•	Alternate Model Variant: If our DistilBERT model underperforms or we suspect a better architecture could boost accuracy, try an alternative like distilroberta-base (which is a distilled RoBERTa model). RoBERTa is known to be a robust encoder, so this variant might yield a slight improvement. We would fine-tune it similarly (maybe at a shorter max_length like 384 to compensate for any increased memory use) and compare validation accuracy. If it consistently outperforms DistilBERT by a meaningful margin (say a few percentage points), we might switch to this model for the final solution. Otherwise, we stick with DistilBERT for efficiency.
	•	Incorporating n_ingredients Feature: If the model is struggling to differentiate certain chefs and we suspect the number of ingredients might help (for instance, maybe one chef consistently uses a high number of ingredients), we could incorporate this. The quick way is to add a token in the text like "NumIngredients: X" in the recipe text so the model sees that information. A more complex way is to extend the model’s classifier head to accept an extra scalar input (through a small fully connected layer). We will only do this if we have evidence it could help (and if time permits), since it adds complexity.
	•	Tuning Hyperparameters: If the model is just shy of our target, we might do minor tuning – e.g., try a slightly higher learning rate or more epochs (with caution to not overfit), or enabling class weights in the loss for the most imbalanced classes. Since our initial choices are fairly standard, we don’t expect to need extensive tuning, but it’s an available step.
Outcome: Any tweaks or experiments either confirm that our initial pipeline is sufficient or provide a modest improvement. We will have explored the “what-ifs” from teammates’ suggestions in a limited way (e.g., showing that adding a numeric feature or using a bigger model yields little gain unless needed). By the end of this stage, we should have our final model choice locked down, with all key decisions justified by evidence.

Stage 7: Final Model Training and Testing

Tasks: Once we are satisfied with the pipeline, we will train the final model on the combined training + validation data (if allowed/appropriate) or just the full training set, and then evaluate on the held-out test set (if a separate test is provided for submission). Use the same hyperparameters that worked best. This final training ensures the model has as much data as possible before making predictions. After training, run the model on the test set to generate predictions for each recipe’s chef. We’ll format the output as required (e.g., a results.txt or a CSV with recipe IDs and predicted chef labels). Double-check the output format against project requirements.
Outcome: A set of final predictions ready for submission or evaluation, and a final trained model. We will also save this model checkpoint and any artifacts (so we can later demonstrate it or further analyze it if needed).

Stage 8: Reporting and Wrap-Up

Tasks: Compose the final 2-page report summarizing our approach and results. This write-up will be in plain English, highlighting: the problem and why it’s challenging (mention class imbalance, etc.), our chosen solution (DistilBERT text classifier), and the rationale for each design choice (drawing from the analysis and results we gathered). We’ll include the baseline vs. fine-tuned model results to show improvement, and mention the ablation insights (e.g., “including recipe steps improved accuracy by X points, validating our use of the full text”). We will also discuss any trade-offs (speed vs. accuracy, complexity vs. performance) and how our approach meets the requirements (fast, works on Mac, not over-engineered). Any interesting findings (like which chefs were hardest to classify) can be noted. Finally, we ensure the code is organized (as per the sketched repository structure) and all instructions to run the training and inference are in the README.
Outcome: A complete project deliverable: code, final model, predictions, and a concise report. This stage makes sure we can confidently defend our pipeline as the “best of all worlds” solution, backed by both reasoning and experimental evidence.

⸻

Summary: We have validated that the proposed pipeline is well-founded and balances both performance and practicality. The implementation will proceed in clear phases – starting from data prep and baseline, through model fine-tuning and evaluation, then iterative improvements and final deployment. By following this plan, we ensure that we cover the essentials (and a few stretch goals) within our timeframe, and we’ll be able to demonstrate a successful chef classification model that outperforms the baseline by leveraging modern NLP techniques.
